{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709ccd9b",
   "metadata": {},
   "source": [
    "# Candy Crush Playstore Review Sentiment Analysis\n",
    "\n",
    "This notebook performs text classification on Candy Crush reviews from the Google Play Store.\n",
    "\n",
    "**Tasks:**\n",
    "1. Data Cleaning - Remove reviewId column\n",
    "2. Text Normalization - Cleaning, tokenization, lemmatization\n",
    "3. Text Classification - Train 3 models and select the best one\n",
    "4. Performance Evaluation - Display predictions and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ae3e2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1f46cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe674ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data...\n",
      "NLTK data downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "print(\"Downloading required NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "print(\"NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ccd96",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8f4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (10000, 3)\n",
      "Columns: ['reviewId', 'content', 'score']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2225e5c9-4669-4a0b-a603-ad1216782993</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7912f6e-94e6-4eee-8ca0-0e7d052c59a4</td>\n",
       "      <td>Great game</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>618d3a26-cfec-4e49-a38c-c128f1945552</td>\n",
       "      <td>This game is addicting and exciting.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>debdb003-524c-41db-98d3-fe7d7c94b55c</td>\n",
       "      <td>Best game üéØ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d9c3b18-023c-48c7-9d17-8d57cd7f8d2a</td>\n",
       "      <td>we need to get paid to play, I noticed some up...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId  \\\n",
       "0  2225e5c9-4669-4a0b-a603-ad1216782993   \n",
       "1  c7912f6e-94e6-4eee-8ca0-0e7d052c59a4   \n",
       "2  618d3a26-cfec-4e49-a38c-c128f1945552   \n",
       "3  debdb003-524c-41db-98d3-fe7d7c94b55c   \n",
       "4  5d9c3b18-023c-48c7-9d17-8d57cd7f8d2a   \n",
       "\n",
       "                                             content  score  \n",
       "0                                               good      5  \n",
       "1                                         Great game      5  \n",
       "2               This game is addicting and exciting.      5  \n",
       "3                                        Best game üéØ      5  \n",
       "4  we need to get paid to play, I noticed some up...      3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/Candy Crush Saga.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c3211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing reviewId column: (10000, 2)\n",
      "\n",
      "Columns: ['content', 'score']\n"
     ]
    }
   ],
   "source": [
    "# Remove reviewId column\n",
    "df = df.drop('reviewId', axis=1)\n",
    "print(f\"After removing reviewId column: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846e6521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "content    0\n",
      "score      0\n",
      "dtype: int64\n",
      "\n",
      "After removing missing values: (10000, 2)\n",
      "\n",
      "Score distribution:\n",
      "score\n",
      "1     750\n",
      "2     226\n",
      "3     384\n",
      "4    1031\n",
      "5    7609\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Remove rows with missing values\n",
    "df = df.dropna()\n",
    "print(f\"\\nAfter removing missing values: {df.shape}\")\n",
    "\n",
    "# Display score distribution\n",
    "print(f\"\\nScore distribution:\\n{df['score'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298955d1",
   "metadata": {},
   "source": [
    "## 3. Text Normalization\n",
    "\n",
    "We'll perform the following text processing steps:\n",
    "- Convert to lowercase\n",
    "- Remove URLs, special characters, and numbers\n",
    "- Tokenization\n",
    "- Remove stopwords\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e49e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Text processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c81d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text cleaning...\n",
      "Applying tokenization and lemmatization...\n",
      "After removing empty processed content: (9633, 4)\n",
      "\n",
      "Sample processed reviews:\n",
      "\n",
      "Original: good\n",
      "Processed: good\n",
      "\n",
      "Original: Great game\n",
      "Processed: great game\n",
      "\n",
      "Original: This game is addicting and exciting.\n",
      "Processed: game addicting exciting\n"
     ]
    }
   ],
   "source": [
    "# Apply text cleaning\n",
    "print(\"Applying text cleaning...\")\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Apply tokenization and lemmatization\n",
    "print(\"Applying tokenization and lemmatization...\")\n",
    "df['processed_content'] = df['cleaned_content'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Remove empty processed content\n",
    "df = df[df['processed_content'].str.len() > 0]\n",
    "print(f\"After removing empty processed content: {df.shape}\")\n",
    "\n",
    "# Display sample processed reviews\n",
    "print(\"\\nSample processed reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['content'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['processed_content'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c89359",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0103c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7706\n",
      "Test set size: 1927\n"
     ]
    }
   ],
   "source": [
    "# Features and target\n",
    "X = df['processed_content']\n",
    "y = df['score']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69563f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying TF-IDF Vectorization...\n",
      "TF-IDF feature matrix shape: (7706, 1000)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "print(\"Applying TF-IDF Vectorization...\")\n",
    "tfidf = TfidfVectorizer(max_features=1000, min_df=2)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb37c82",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "We'll train 3 different models:\n",
    "1. Logistic Regression\n",
    "2. Multinomial Naive Bayes\n",
    "3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106d7b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 1: LOGISTIC REGRESSION\n",
      "================================================================================\n",
      "Accuracy:  0.7867\n",
      "Precision: 0.6521\n",
      "Recall:    0.7867\n",
      "F1-Score:  0.7116\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_predictions = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "lr_precision = precision_score(y_test, lr_predictions, average='weighted', zero_division=0)\n",
    "lr_recall = recall_score(y_test, lr_predictions, average='weighted', zero_division=0)\n",
    "lr_f1 = f1_score(y_test, lr_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': lr_accuracy,\n",
    "    'precision': lr_precision,\n",
    "    'recall': lr_recall,\n",
    "    'f1_score': lr_f1,\n",
    "    'predictions': lr_predictions\n",
    "}\n",
    "\n",
    "print(f\"Accuracy:  {lr_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall:    {lr_recall:.4f}\")\n",
    "print(f\"F1-Score:  {lr_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e56b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 2: MULTINOMIAL NAIVE BAYES\n",
      "================================================================================\n",
      "Accuracy:  0.7914\n",
      "Precision: 0.6595\n",
      "Recall:    0.7914\n",
      "F1-Score:  0.7191\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 2: MULTINOMIAL NAIVE BAYES\")\n",
    "print(\"=\"*80)\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_predictions = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_precision = precision_score(y_test, nb_predictions, average='weighted', zero_division=0)\n",
    "nb_recall = recall_score(y_test, nb_predictions, average='weighted', zero_division=0)\n",
    "nb_f1 = f1_score(y_test, nb_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "results['Naive Bayes'] = {\n",
    "    'accuracy': nb_accuracy,\n",
    "    'precision': nb_precision,\n",
    "    'recall': nb_recall,\n",
    "    'f1_score': nb_f1,\n",
    "    'predictions': nb_predictions\n",
    "}\n",
    "\n",
    "print(f\"Accuracy:  {nb_accuracy:.4f}\")\n",
    "print(f\"Precision: {nb_precision:.4f}\")\n",
    "print(f\"Recall:    {nb_recall:.4f}\")\n",
    "print(f\"F1-Score:  {nb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2caeffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 3: RANDOM FOREST\n",
      "================================================================================\n",
      "Accuracy:  0.7774\n",
      "Precision: 0.6800\n",
      "Recall:    0.7774\n",
      "F1-Score:  0.7124\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 3: RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "rf_precision = precision_score(y_test, rf_predictions, average='weighted', zero_division=0)\n",
    "rf_recall = recall_score(y_test, rf_predictions, average='weighted', zero_division=0)\n",
    "rf_f1 = f1_score(y_test, rf_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'accuracy': rf_accuracy,\n",
    "    'precision': rf_precision,\n",
    "    'recall': rf_recall,\n",
    "    'f1_score': rf_f1,\n",
    "    'predictions': rf_predictions\n",
    "}\n",
    "\n",
    "print(f\"Accuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall:    {rf_recall:.4f}\")\n",
    "print(f\"F1-Score:  {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88da65",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf323bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL COMPARISON\n",
      "================================================================================\n",
      "              Model  Accuracy  Precision   Recall  F1-Score\n",
      "Logistic Regression  0.786715   0.652108 0.786715  0.711590\n",
      "        Naive Bayes  0.791386   0.659484 0.791386  0.719107\n",
      "      Random Forest  0.777374   0.679989 0.777374  0.712412\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL: Naive Bayes\n",
      "F1-Score: 0.7191\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "    'Precision': [results[model]['precision'] for model in results.keys()],\n",
    "    'Recall': [results[model]['recall'] for model in results.keys()],\n",
    "    'F1-Score': [results[model]['f1_score'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model based on F1-score\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d483db",
   "metadata": {},
   "source": [
    "## 7. Detailed Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a50a0697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Evaluation of Naive Bayes\n",
      "================================================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.54      0.54       147\n",
      "           2       0.00      0.00      0.00        44\n",
      "           3       0.00      0.00      0.00        74\n",
      "           4       0.00      0.00      0.00       200\n",
      "           5       0.82      0.99      0.89      1462\n",
      "\n",
      "    accuracy                           0.79      1927\n",
      "   macro avg       0.27      0.31      0.29      1927\n",
      "weighted avg       0.66      0.79      0.72      1927\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  79    0    1    1   66]\n",
      " [  20    0    0    0   24]\n",
      " [  21    0    0    2   51]\n",
      " [  14    0    0    0  186]\n",
      " [  14    0    0    2 1446]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Detailed Evaluation of {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9fa26",
   "metadata": {},
   "source": [
    "## 8. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ed3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 10 sample predictions:\n",
      "\n",
      "================================================================================\n",
      "‚úì Review: Charger booster is a powerful weapon for us. Sir ‚ò∫Ô∏è‚ò∫Ô∏è thank ...\n",
      "  Actual: 5 | Predicted: 5\n",
      "\n",
      "‚úì Review: good...\n",
      "  Actual: 5 | Predicted: 5\n",
      "\n",
      "‚úì Review: this app literally peak bro since earlier this app has relea...\n",
      "  Actual: 5 | Predicted: 5\n",
      "\n",
      "‚úì Review: so stupid üôÑüòÇ...\n",
      "  Actual: 1 | Predicted: 1\n",
      "\n",
      "‚úì Review: excellent...\n",
      "  Actual: 5 | Predicted: 5\n",
      "\n",
      "‚úì Review: levels are very enjoy but very tough...\n",
      "  Actual: 5 | Predicted: 5\n",
      "\n",
      "‚úì Review: nice game...\n",
      "  Actual: 5 | Predicted: 5\n",
      "\n",
      "‚úì Review: good game...\n",
      "  Actual: 5 | Predicted: 5\n",
      "\n",
      "‚úó Review: good...\n",
      "  Actual: 4 | Predicted: 5\n",
      "\n",
      "‚úì Review: Levels is too low and notifications is over so i will uninst...\n",
      "  Actual: 1 | Predicted: 1\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show sample predictions\n",
    "sample_size = min(10, len(X_test))\n",
    "print(f\"Showing {sample_size} sample predictions:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    actual = y_test.iloc[i]\n",
    "    predicted = best_predictions[i]\n",
    "    original_text = df.loc[y_test.index[i], 'content']\n",
    "    \n",
    "    status = \"‚úì\" if actual == predicted else \"‚úó\"\n",
    "    print(f\"{status} Review: {original_text[:60]}...\")\n",
    "    print(f\"  Actual: {actual} | Predicted: {predicted}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0b7a3",
   "metadata": {},
   "source": [
    "## 9. Predict on Unseen Custom Data\n",
    "\n",
    "Now you can test the model with your own review text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b4cc217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom prediction function created!\n",
      "Best model selected: Naive Bayes\n"
     ]
    }
   ],
   "source": [
    "def predict_custom_review(review_text, model_name='best'):\n",
    "    \"\"\"\n",
    "    Predict sentiment score for a custom review\n",
    "    \n",
    "    Parameters:\n",
    "    - review_text: string, the review to predict\n",
    "    - model_name: 'best', 'Logistic Regression', 'Naive Bayes', or 'Random Forest'\n",
    "    \n",
    "    Returns:\n",
    "    - predicted score (1-5)\n",
    "    \"\"\"\n",
    "    # Use best model by default\n",
    "    if model_name == 'best':\n",
    "        model_name = best_model_name\n",
    "        model = results[best_model_name]\n",
    "    else:\n",
    "        model = results[model_name]\n",
    "    \n",
    "    # Get the appropriate model's predictions\n",
    "    if model_name == 'Logistic Regression':\n",
    "        current_model = lr_model\n",
    "    elif model_name == 'Naive Bayes':\n",
    "        current_model = nb_model\n",
    "    else:  # Random Forest\n",
    "        current_model = rf_model\n",
    "    \n",
    "    # Preprocess the text\n",
    "    cleaned = clean_text(review_text)\n",
    "    processed = tokenize_and_lemmatize(cleaned)\n",
    "    \n",
    "    # Transform using the same TF-IDF vectorizer\n",
    "    vectorized = tfidf.transform([processed])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = current_model.predict(vectorized)[0]\n",
    "    \n",
    "    # Get probability scores if available\n",
    "    if hasattr(current_model, 'predict_proba'):\n",
    "        probabilities = current_model.predict_proba(vectorized)[0]\n",
    "        prob_dict = {score: prob for score, prob in zip(current_model.classes_, probabilities)}\n",
    "    else:\n",
    "        prob_dict = None\n",
    "    \n",
    "    return prediction, prob_dict\n",
    "\n",
    "print(\"Custom prediction function created!\")\n",
    "print(f\"Best model selected: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79cd3ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDICTIONS ON CUSTOM UNSEEN DATA\n",
      "================================================================================\n",
      "\n",
      "1. Review: \"This game is absolutely amazing! I love playing it every day!\"\n",
      "   Predicted Score: 5/5\n",
      "   Confidence Distribution:\n",
      "      Score 1: 1.23% \n",
      "      Score 2: 0.25% \n",
      "      Score 3: 0.63% \n",
      "      Score 4: 5.64% ‚ñà‚ñà\n",
      "      Score 5: 92.25% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "2. Review: \"Worst game ever. Too many ads and pay to win.\"\n",
      "   Predicted Score: 1/5\n",
      "   Confidence Distribution:\n",
      "      Score 1: 57.11% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      Score 2: 2.68% ‚ñà\n",
      "      Score 3: 5.46% ‚ñà‚ñà\n",
      "      Score 4: 9.66% ‚ñà‚ñà‚ñà‚ñà\n",
      "      Score 5: 25.09% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "3. Review: \"It's okay, nothing special but kills time\"\n",
      "   Predicted Score: 5/5\n",
      "   Confidence Distribution:\n",
      "      Score 1: 5.80% ‚ñà‚ñà\n",
      "      Score 2: 4.08% ‚ñà‚ñà\n",
      "      Score 3: 4.60% ‚ñà‚ñà\n",
      "      Score 4: 20.67% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      Score 5: 64.85% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "4. Review: \"Great graphics and fun gameplay\"\n",
      "   Predicted Score: 5/5\n",
      "   Confidence Distribution:\n",
      "      Score 1: 0.99% \n",
      "      Score 2: 0.76% \n",
      "      Score 3: 2.88% ‚ñà\n",
      "      Score 4: 10.76% ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      Score 5: 84.61% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "5. Review: \"Terrible experience, keeps crashing\"\n",
      "   Predicted Score: 5/5\n",
      "   Confidence Distribution:\n",
      "      Score 1: 28.44% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      Score 2: 7.10% ‚ñà‚ñà‚ñà\n",
      "      Score 3: 8.62% ‚ñà‚ñà‚ñà‚ñà\n",
      "      Score 4: 23.51% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      Score 5: 32.32% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with your own custom reviews!\n",
    "# Replace the text below with your own review\n",
    "\n",
    "custom_reviews = [\n",
    "    \"This game is absolutely amazing! I love playing it every day!\",\n",
    "    \"Worst game ever. Too many ads and pay to win.\",\n",
    "    \"It's okay, nothing special but kills time\",\n",
    "    \"Great graphics and fun gameplay\",\n",
    "    \"Terrible experience, keeps crashing\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREDICTIONS ON CUSTOM UNSEEN DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(custom_reviews, 1):\n",
    "    prediction, probabilities = predict_custom_review(review)\n",
    "    \n",
    "    print(f\"\\n{i}. Review: \\\"{review}\\\"\")\n",
    "    print(f\"   Predicted Score: {prediction}/5\")\n",
    "    \n",
    "    if probabilities:\n",
    "        print(f\"   Confidence Distribution:\")\n",
    "        for score in sorted(probabilities.keys()):\n",
    "            bar = \"‚ñà\" * int(probabilities[score] * 50)\n",
    "            print(f\"      Score {score}: {probabilities[score]:.2%} {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ded71f",
   "metadata": {},
   "source": [
    "### Try Your Own Single Review\n",
    "\n",
    "Modify the review text below and run the cell to get a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66dc769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "YOUR CUSTOM REVIEW PREDICTION\n",
      "================================================================================\n",
      "\n",
      "Review: \"This game is the worst! I can't stand the constant ads and glitches.\"\n",
      "\n",
      "üéØ Predicted Score: 1/5\n",
      "üìä Model Used: Naive Bayes\n",
      "\n",
      "üìà Confidence Distribution:\n",
      "   Score 1: 51.52% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Score 2:  4.50% ‚ñà‚ñà\n",
      "   Score 3:  7.24% ‚ñà‚ñà‚ñà\n",
      "   Score 4: 16.39% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Score 5: 20.34% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ‚úèÔ∏è EDIT THIS: Put your own review here!\n",
    "my_review = \"This game is the worst! I can't stand the constant ads and glitches.\"\n",
    "\n",
    "# Make prediction\n",
    "predicted_score, probabilities = predict_custom_review(my_review)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"YOUR CUSTOM REVIEW PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nReview: \\\"{my_review}\\\"\")\n",
    "print(f\"\\nüéØ Predicted Score: {predicted_score}/5\")\n",
    "print(f\"üìä Model Used: {best_model_name}\")\n",
    "\n",
    "if probabilities:\n",
    "    print(f\"\\nüìà Confidence Distribution:\")\n",
    "    for score in sorted(probabilities.keys()):\n",
    "        bar = \"‚ñà\" * int(probabilities[score] * 50)\n",
    "        percentage = probabilities[score] * 100\n",
    "        print(f\"   Score {score}: {percentage:5.2f}% {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925848fa",
   "metadata": {},
   "source": [
    "## 10. Save Model for Backend API\n",
    "\n",
    "Save the trained model and vectorizer to use in the Flask backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d29952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL SAVED SUCCESSFULLY!\n",
      "================================================================================\n",
      "Model: Naive Bayes\n",
      "Saved to: ../backend/models/\n",
      "Files created:\n",
      "  - best_model.pkl\n",
      "  - tfidf_vectorizer.pkl\n",
      "  - model_metadata.pkl\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = '../backend/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Determine which model to save (the best one)\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    best_model = lr_model\n",
    "elif best_model_name == 'Naive Bayes':\n",
    "    best_model = nb_model\n",
    "else:  # Random Forest\n",
    "    best_model = rf_model\n",
    "\n",
    "# Save the model\n",
    "with open(f'{models_dir}/best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open(f'{models_dir}/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'accuracy': results[best_model_name]['accuracy'],\n",
    "    'f1_score': results[best_model_name]['f1_score']\n",
    "}\n",
    "\n",
    "with open(f'{models_dir}/model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Saved to: {models_dir}/\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - best_model.pkl\")\n",
    "print(f\"  - tfidf_vectorizer.pkl\")\n",
    "print(f\"  - model_metadata.pkl\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
